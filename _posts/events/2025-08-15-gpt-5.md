> id: https://www.36kr.com/p/3423728239086985

> link: https://www.36kr.com/p/3423728239086985

> title: GPT-5

# GPT-5
_Published on Fri, 15 Aug 2025 07:20:21 GMT_

GPT-5正式发布，虽然在测试集上登顶，但用户反馈却褒贬不一，不少用户希望能保留GPT-4o。OpenAI希望通过增加模型路由功能，以不同模型，不同算力成本满足不同用户需求的目标。

就目前的体验来看，OpenAI想要的“统一模型”的努力还任重道远。而GPT-5没有出现模型能力的显著突破和技术范式的更新，OpenAI做的更多是产品化创新——GPT-5是一个幻觉更少，更易用，能帮用户解决更多具体问题的模型，但是没有新能力，也没有彻底解决大模型的某个结构性缺陷。而近日，有外媒报道DeepSeek正在用国产芯片训练最新的模型，但是新模型的发布日期依然不定。GPT-5的发布似乎表明，大模型能力上限疑似撞墙。在这堵“Transformer能力边界之墙”面前，OpenAI选择了将现有能力产品化到极致，将“超级APP”的叙事进行到底。而DeepSeek在追求模型上限的竞争压力变缓时，正在开启“自给自足”的支线任务。

一心要用AGI将人类社会带入“极度富足”状态的OpenAI在做超级APP的路上渐行渐远，营收和估值一路飙升；而希望探索AI能力上限，搭建开源生态推进技术普惠的DeepSeek，需要解的可能是不同的题目。

也许多年后，当人们回顾大模型行业发展的时间线时会发现，多条线路相交于DeepSeek R1和GPT-4o发布，分化于GPT-5之后。

**01 性能霸榜却未达预期的GPT-5，加速产品化**
----------------------------

市场期待的是一次范式转移，一次足以重新定义人机交互的时刻。但**最终的结果，更像是一次常规的升级。**它的模型参数更多，训练数据更广，在一些基准测试中得分更高，但它在核心的智能层面，并未展现出革命性的进步。纽约大学名誉教授加里·马库斯曾用三个词来概括GPT-5的表现：“姗姗来迟、过度炒作、平庸无奇”。

他的分析指出，GPT-5未能根除大型语言模型固有的缺陷。它仍然会在某些时候编造事实，即所谓的“幻觉”问题。在面对需要多步逻辑推理的任务时，它仍然会犯错。在提供现实世界的理解的多模态性能上，也没有什么质的提升。

这些问题在GPT-4时代就存在，业界曾希望GPT-5能提供解决方案，但现实是OpenAI选择了对现有框架进行修补和优化。然后在此能力基础之上提供一个产品化更好，更加易用的模型工具。

![](https://img.36krcdn.com/hsossms/20250815/v2_a3100842ee4940a7bd47ebf6969a7bb1@000000_oswg361124oswg828oswg627_img_000?x-oss-process=image/format,jpg/interlace,1)

**如果说核心智能的停滞是技术专家和深度用户的感受，那么其在多模态能力上有限的进步，则让技术爱好者感到失望。**在GPT-5发布之前，一个普遍的共识是，下一代人工智能的决胜场将是多模态。人们想象中的GPT-5应该能像人类一样，无缝地接收、理解和融合处理来自文本、图像、音频、视频等多种渠道的信息。然而，现实中的GPT-5在多模态交互上的表现，更像是一个经过优化的GPT-4V。它能精准地完成描述性任务，比如识别照片中的物体，但一旦任务转向理解，它的能力边界就显现出来。

作为将Transformer算法能力和语言最早进行结合，用ChatGPT开创了大模型时代，又将强化学习有机地融入大模型的训练过程中，捅破了大模型推理能力天花板的OpenAI，一直以来都是业界的标杆。但是GPT-5发布之后，除去性能上的“未达预期”之外，获得获得外界关注的特性似乎都是产品级别的变化。

OpenAI希望通过“模型路由”功能来让用户避免在众多模型中进行选择，降低新用户的使用门槛，同时也能合理化算力分配，使得OpenAI能够在有限的算力资源下为更多的用户提供更加高质量的服务。

按照OpenAI的说法，虽然GPT-5大幅降低了模型的幻觉，但是对于一些基础的数理问题和对现实世界的理解上，表现难以说得上令人满意，依然会出现很多明显的错误。相反，可能由于训练数据中用了更多的生产力相关内容，在情商上还有很大的退步，引得普通聊天用户用取关来威胁GPT-4o的“返场”。

GPT-5表现出OpenAI在大模型能力突破的“躺平”，几乎间接宣布了“大模型能力墙”已经到来，或者至少是大模型技术突破暂时进入了平缓期。未来模型能力能否重新回到“GPT-3到GPT-4o”这样的“蒙眼狂奔”的快车道，依赖于研究人员在底层技术上的突破和创新。

OpenAI前首席科学家Ilya曾经在2023年底的“Why next-token prediction is enough for AGI”访谈中对于AI技术发展趋势的总结似乎某种程度上预言了这一刻的到来。

![](https://img.36krcdn.com/hsossms/20250815/v2_4788c9ad76814b9e9db306df117fa539@000000_oswg387757oswg829oswg425_img_000?x-oss-process=image/format,jpg/interlace,1)

“不同的研究人员和项目会在一个时间段内有不同的方向，然后当人们发现了一个技术有效之后，研究会向那个方向快速收敛，之后可能又会回归到之前百花争鸣的状态”

**02 梁文锋能否抓住时机，完成国产大模型的“自给自足”**
-------------------------------

如果Transformer技术墙真的已经到来，我们对DeepSeek还能有什么合理的期待？纵观DeepSeek的产品发布历史，每一个重量级的发布，都在它自己的时间线上解决了大模型技术上某个重要问题。

2024年5月的DeepSeek-V2系列则革命性地处理了长上下文处理的效率问题，首创多头潜注意力（MLA）机制，支持高达128K token的处理，同时以极低的API定价（每百万token 2元人民币）引发了中国AI巨头的价格战，显著提升了大模型的可负担性和实际部署潜力。

2024年12月的DeepSeek-V3以671B参数的MoE架构登场，针对推理速度的痛点实现了每秒60token的3倍加速，性能达到GPT-4o的同时保持资源高效，以一己之力几乎拉平了开源模型和闭源模型性能上的差距。

2025年1月的DeepSeek-R1专注于推理能力的提升，在AIME和MATH任务上匹敌或超越OpenAI的o1模型，成本远远低于当时的所有模型，通过App登顶美国App Store，解决了高端AI的访问壁垒问题，加速了开源AI的全球普及与民主化。

而在V3和R1让DeepSeek彻底出圈之后，它似乎也从一家发源于量化，成名于大模型，变成了一家肩负起了更多使命的科技公司。

根据外媒报道， **DeepSeek目前正在将最先进大模型的训练转移到国产芯片之上。**大模型的国产化之路，远比普通人想象的要困难。但是在不稳定的地缘政治等各种因素的影响之下，如果没有办法摆脱对英伟达GPU的依赖，所有中国AI公司的头上，永远悬挂着一把达摩克里斯之剑。

而此时OpenAI发布的GPT-5，暗示了以Transformer为核心的大模型技术，发展曲线暂时变缓。这给了包括DeepSeek在内的所有科技公司一个信号——可以在不断稳定提升模型性能的主线任务之外，放心地点开其他支线了。

而要实现前沿性能大模型从训练到推理的国产化，即便对于一家已经将大模型研发从“原子弹变成茶叶蛋”的顶尖AI公司来说，难度不亚于再研发一种全新的原子弹。这个过程中需要解决的技术问题，可能比训练DeepSeek之前发布的所有模型需要攻克的难题加起来还要多得多。

首先是国产GPU本身性能和英伟达的GPU的单卡性能相比依然还有接近代际的差距。即便是国产GPU已经能通过更密集的互联技术将单卡的性能差距尽力弥补。但是要和硅谷大模型采用的英伟达“10万卡集群”竞争，采用国产GPU训练性能最顶尖的模型，需要面对难以想象的工程难题。

大模型研发离不开像PyTorch或TensorFlow这样的开源框架，这些框架原本是为国际主流硬件优化的。如果DeepSeek要国产化，就得把整个软件栈迁移到本土硬件上，这意味着要重写或修改大量的代码来兼容本土的计算架构。和发展了多年的成熟主流开源框架和CUDA生态相比，重构的国产软件栈要在性能和稳定性上接近已经发展近10年的主流解决方案，难度也相当大。

但如果DeepSeek能和国产硬件厂商持续密切配合，像DeepSeek将大模型的研发一样，从零开始一步步行至行业最前沿，才有希望彻底解下头顶那柄达摩克里斯之剑。

在持续改进大模型训练和推理效率这个方向上，DeepSeek也依然在持续探索，取得了令人瞩目的成绩。

今年7月底，由DeepSeek团队和北京大学等机构发表，梁文锋作为通讯作者的论文《Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention》获得ACL 2025最佳论文奖。

![](https://img.36krcdn.com/hsossms/20250815/v2_7f7db4bdddc94df481eed884ade08d31@000000_oswg156791oswg831oswg329_img_000?x-oss-process=image/format,jpg/interlace,1)

https://arxiv.org/abs/2502.11089

这篇论文首次将**稀疏注意力从理论推理带进了完整训练流程，**在保持模型性能，提高训练效率的同时，还带来了高达 11 倍的推理加速。在自然语言处理顶会ACL上获得最佳论文奖，足以说明业界对于这项技术含金量的认可。

愿意将这样在商业竞争中起到关键作用的创新公开，也体现了DeepSeek不断推进大模型技术普惠的决心和能力。

让我们拭目以待，融合了更多像“原生稀疏注意力”这样的DeepSeek新模型将会在能力和效率上带给业界多大惊喜，又能将大模型研发的国产化程度，推动到哪里。

本文来自微信公众号 [“直面AI”（ID：faceaibang）](https://mp.weixin.qq.com/s?__biz=MjM5OTQzMTAxOA==&mid=2450408942&idx=1&sn=91159dbfbc5822e5a591235252bc9609&chksm=b114723abd8edfcb3c000276bce3a048ca8402f25d2668f094546d785837d7ea9a6f9369e268&scene=0&xtrack=1#rd)，作者：胡润 苗正，36氪经授权发布。
